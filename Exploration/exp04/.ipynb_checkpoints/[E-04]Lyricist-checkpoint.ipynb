{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e17d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "txt_list = glob('./data/lyrics/*')\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2869ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['They say get ready for the revolution', \"I think it's time we find some sorta solution\", \"Somebody's caught up in the endless pollution\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29b6d818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They say get ready for the revolution', \"I think it's time we find some sorta solution\", \"Somebody's caught up in the endless pollution\", 'They need to wake up, stop living illusions I know you need to hear this', \"Why won't somebody feel this\", 'This is my wish that we all feel connected', 'This is my wish that nobodies neglected Be like a rocket baby', 'Be like a rocket Take off', 'Just fly, away (ay, ay)', 'To find your space Take off', 'Just fly, away (ay, ay)', 'To find your place Take off You know what they say about mixing the races', 'And in the end we got the same faces', 'My mama told me got love yourself first', 'And if you disagree, get off this damn earth I want to feel connected', \"Don't want to be neglected\", 'This is my wish that we all find our places', 'This is my wish that we all escalate (yeah) Be like a rocket baby', 'Be like a rocket Take off', 'Just fly, away (ay, ay)', 'To find your space Take off', 'Just fly, away (ay, ay)', 'To find your space Take off', 'Just fly, away (ay, ay)', 'To find your space Take off', 'Just fly, away (ay, ay)', 'To find your space Take off [?] see everything', \"Hear this and you'll want to sing\", 'Anywhere you wanna', 'Everything you wanna', 'All you will have to is be', 'Think it and you can achieve', \"Everything's not what it seems\", 'Anywhere you wanna', 'Everything you wanna', 'All you will have to is be Like a rocket', 'Just fly, away (ay, ay)', 'To find your space Take off', 'Just fly, away (ay, ay)', 'To find your space Take off, take off', 'Be like a rocket baby', 'Be like a rocket', 'Take off, take off', 'Be like a rocket baby', 'Be like a rocket All the people in the crowd', 'Grab a partner, take it down', \"It's just me against the music, its just me and me, yeah come on Hey Britney, are you ready, uh-huh, are you? When no one cares\", \"It's whipping my hair\", \"It's pulling my waist\", 'To hell with stares', 'The sweat is dripping all over my face', \"And no one's there\", \"I'm the only one dancing up in this place\", \"Tonight I'm here\", 'To the beat of the drum, gotta give it that bass I want to get stupid', 'Trying to take my music', \"It's like a competition\", 'Me against the beat', 'I want to get in the zone', 'I want to get in the zone If you really want to party', 'Send my love and get ya', 'Really to hit, you could die', \"In a minute, I'ma take-a ya on, I'ma take-a ya on\", '(Hey hey hey) All my people on the floor let me see you dance', 'All my people up for more let me see you dance', \"All my people, 'round and 'round let me see you dance\", 'All my people in the crowd, let me see you dance', '(Wont you let me got to show just take on the zone)', \"Let's take on the zone, lets take on the zone\", '(If you really want to battle)', \"(It's you and me baby and the music turns our body all night long)\", \"All night long (We're almost there)\", \"I'm feeling it bad and I can't explain\", 'My soul is there', 'My hips all moving at a rapid pace', 'Can you feel it burn', 'From the tip of my toes, running through my veins', \"And now's your turn\", \"Let me see what ya got, don't hesitate I want to get stupid\", 'Trying to take my music', \"It's like a competition\", 'Me against the beat', 'I want to get in the zone', 'I want to get in the zone If you really want to party', 'Settle up and get your rhythm', 'Try to hit, you could die', \"In a minute, I'ma take-a ya on, I'ma take-a ya on\", 'Hey, hey, hey (here we go) All my people on the floor let me see you dance', 'All my people up for more let me see you dance', \"All my people, 'round and 'round let me see you dance\", 'All my people in the crowd, let me see you dance', 'I wanna see you Get on the floor', 'Baby, lose control', 'Just work your body', 'And let it go', 'If you want to party', 'Just grab somebody', 'And baby we can dance all night Hey Britney, you say you wanna lose control', 'Come over here I got something to show you', \"Sexy lady, I'd rather see you bare your soul\"]\n"
     ]
    }
   ],
   "source": [
    "print(raw_corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e914943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1\", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a6ffff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =[]\n",
    "\n",
    "for sentence in  raw_corpus:\n",
    "    if len(sentence) == 0 or sentence[-1] == ']':\n",
    "        continue\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab79f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a7cf17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 12000,\n",
    "        filters = ' ',\n",
    "        oov_token = \"<unk>\" \n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensors = []\n",
    "    for i in range(len(tensor)):\n",
    "        if len(tensor[i]) <= 15:\n",
    "            tensors.append(tensor[i])\n",
    "    tensors = np.array(tensors)\n",
    "    tensors = tf.keras.preprocessing.sequence.pad_sequences(tensors, padding = 'post')\n",
    "    return tensors, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "46880012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_423/1896910369.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensors = np.array(tensors)\n"
     ]
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f36e7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8055e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63b04cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "befe8993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124124, 14)\n",
      "Target Train: (124124, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69c52d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5e103b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset =  tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8e68c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "33405300",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(VOCAB_SIZE, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "44087663",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(loss = loss, optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a02f29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "484/484 [==============================] - 245s 465ms/step - loss: 3.2896\n",
      "Epoch 2/10\n",
      "484/484 [==============================] - 225s 464ms/step - loss: 2.7211\n",
      "Epoch 3/10\n",
      "484/484 [==============================] - 225s 465ms/step - loss: 2.3905\n",
      "Epoch 4/10\n",
      "484/484 [==============================] - 225s 465ms/step - loss: 2.0672\n",
      "Epoch 5/10\n",
      "484/484 [==============================] - 225s 465ms/step - loss: 1.7654\n",
      "Epoch 6/10\n",
      "484/484 [==============================] - 225s 465ms/step - loss: 1.5050\n",
      "Epoch 7/10\n",
      "484/484 [==============================] - 226s 466ms/step - loss: 1.2990\n",
      "Epoch 8/10\n",
      "484/484 [==============================] - 226s 466ms/step - loss: 1.1501\n",
      "Epoch 9/10\n",
      "484/484 [==============================] - 225s 465ms/step - loss: 1.0601\n",
      "Epoch 10/10\n",
      "484/484 [==============================] - 225s 465ms/step - loss: 1.0111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63b02638b0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e186448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokinizer, init_sentence = \"<start>\", max_len = 20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype = tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis = -1), axis = -1)[:,-1]\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis = 0)], axis = -1)\n",
    "        \n",
    "        if predict_word.numpy()[0] == end_token or test_tensor.shape[1] >= max_len:\n",
    "            break\n",
    "            \n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + ' '\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c77e9957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970/970 [==============================] - 36s 36ms/step - loss: 2.1793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1793322563171387"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(enc_val, dec_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3f0d8607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0a83f930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i am not throwing away my shot <end> '"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i am\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9118e1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you and me together <end> '"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you and \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4e72600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: text_generator/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: text_generator/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('text_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0ac10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
